# Fallback per-token pricing for models where LiteLLM doesn't return cost.
# Used when x-litellm-response-cost header is missing or zero (e.g., local Ollama).
# Prices in USD per 1 million tokens.
models:
  "openai/gpt-4o":
    input_per_1m: 2.50
    output_per_1m: 10.00
  "openai/gpt-4o-mini":
    input_per_1m: 0.15
    output_per_1m: 0.60
  "openai/gpt-4-turbo":
    input_per_1m: 10.00
    output_per_1m: 30.00
  "openai/gpt-3.5-turbo":
    input_per_1m: 0.50
    output_per_1m: 1.50
  "anthropic/claude-sonnet-4-20250514":
    input_per_1m: 3.00
    output_per_1m: 15.00
  "anthropic/claude-3-5-haiku-20241022":
    input_per_1m: 1.00
    output_per_1m: 5.00
  "anthropic/claude-3-opus-20240229":
    input_per_1m: 15.00
    output_per_1m: 75.00
  # Groq models â€” free tier but priced for cost tracking validation.
  # Prices based on Groq's published on-demand rates.
  "groq/llama-4-maverick":
    input_per_1m: 0.50
    output_per_1m: 2.00
  "groq/llama-3.3-70b":
    input_per_1m: 0.59
    output_per_1m: 0.79
  "groq/llama-3.3-70b-versatile":
    input_per_1m: 0.59
    output_per_1m: 0.79
  "groq/llama-3.1-8b":
    input_per_1m: 0.05
    output_per_1m: 0.08
  "groq/llama-3.1-8b-instant":
    input_per_1m: 0.05
    output_per_1m: 0.08
  "groq/gpt-oss-120b":
    input_per_1m: 0.30
    output_per_1m: 0.60
  "groq/qwen3-32b":
    input_per_1m: 0.20
    output_per_1m: 0.40
  "ollama/llama3.2":
    input_per_1m: 0.0
    output_per_1m: 0.0
  "ollama/codellama":
    input_per_1m: 0.0
    output_per_1m: 0.0
