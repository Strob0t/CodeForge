# LiteLLM Proxy Configuration
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# Scenario-based routing: each model is tagged with scenario tags in
# litellm_params.tags. Workers send tags=["think"] (or "default", etc.)
# in their completion requests, and LiteLLM routes to a matching model.
# Supported scenarios: default, background, think, longContext, review, plan

model_list:
  # -- Local Models (Ollama) --
  - model_name: "ollama/*"
    litellm_params:
      model: "ollama/*"
      api_base: "http://host.docker.internal:11434"
      tags: ["default", "background"]

  # -- OpenAI --
  - model_name: "gpt-4o"
    litellm_params:
      model: "openai/gpt-4o"
      tags: ["think", "review", "plan"]

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "openai/gpt-4o-mini"
      tags: ["default", "background"]

  # -- Anthropic --
  - model_name: "claude-opus-4.6"
    litellm_params:
      model: "anthropic/claude-opus-4-6"
      tags: ["think", "review", "plan"]

  - model_name: "claude-sonnet-4.6"
    litellm_params:
      model: "anthropic/claude-sonnet-4-6"
      tags: ["think", "review", "plan"]

  - model_name: "claude-sonnet-4"
    litellm_params:
      model: "anthropic/claude-sonnet-4-20250514"
      tags: ["think", "review", "plan"]

  - model_name: "claude-haiku-4.5"
    litellm_params:
      model: "anthropic/claude-haiku-4-5-20251001"
      tags: ["default", "background"]

  - model_name: "claude-haiku-3.5"
    litellm_params:
      model: "anthropic/claude-3-5-haiku-20241022"
      tags: ["default", "background"]

  # -- Google Gemini --
  - model_name: "gemini-2.5-pro"
    litellm_params:
      model: "gemini/gemini-2.5-pro-preview-06-05"
      tags: ["think", "review", "plan", "longContext"]

  - model_name: "gemini-2.5-flash"
    litellm_params:
      model: "gemini/gemini-2.5-flash-preview-05-20"
      tags: ["default", "background", "longContext"]

  - model_name: "gemini-2.0-flash"
    litellm_params:
      model: "gemini/gemini-2.0-flash"
      tags: ["default", "background"]

  # -- Groq (fast inference) --
  - model_name: "groq/llama-3.3-70b"
    litellm_params:
      model: "groq/llama-3.3-70b-versatile"
      tags: ["default", "think", "review", "plan"]

  - model_name: "groq/llama-3.1-8b"
    litellm_params:
      model: "groq/llama-3.1-8b-instant"
      tags: ["default", "background"]

  - model_name: "groq/llama-4-maverick"
    litellm_params:
      model: "groq/meta-llama/llama-4-maverick-17b-128e-instruct"
      tags: ["think", "review", "plan"]

  - model_name: "groq/llama-4-scout"
    litellm_params:
      model: "groq/meta-llama/llama-4-scout-17b-16e-instruct"
      tags: ["default", "background"]

  - model_name: "groq/gpt-oss-120b"
    litellm_params:
      model: "groq/openai/gpt-oss-120b"
      tags: ["think", "review", "plan"]

  - model_name: "groq/gpt-oss-20b"
    litellm_params:
      model: "groq/openai/gpt-oss-20b"
      tags: ["default", "background"]

  - model_name: "groq/qwen3-32b"
    litellm_params:
      model: "groq/qwen/qwen3-32b"
      tags: ["think", "review", "plan"]

  - model_name: "groq/kimi-k2"
    litellm_params:
      model: "groq/moonshotai/kimi-k2-instruct-0905"
      tags: ["think", "review", "plan"]

  - model_name: "groq/compound"
    litellm_params:
      model: "groq/groq/compound"
      tags: ["think", "review", "plan"]

  - model_name: "groq/compound-mini"
    litellm_params:
      model: "groq/groq/compound-mini"
      tags: ["default", "background"]

  # -- Mistral AI --
  - model_name: "mistral-large"
    litellm_params:
      model: "mistral/mistral-large-latest"
      tags: ["think", "review", "plan"]

  - model_name: "mistral-medium"
    litellm_params:
      model: "mistral/mistral-medium-latest"
      tags: ["think", "review", "plan"]

  - model_name: "mistral-small"
    litellm_params:
      model: "mistral/mistral-small-latest"
      tags: ["default", "background"]

  - model_name: "mistral-tiny"
    litellm_params:
      model: "mistral/mistral-tiny-latest"
      tags: ["default", "background"]

  - model_name: "magistral-medium"
    litellm_params:
      model: "mistral/magistral-medium-latest"
      tags: ["think", "review", "plan"]

  - model_name: "magistral-small"
    litellm_params:
      model: "mistral/magistral-small-latest"
      tags: ["default", "background"]

  - model_name: "codestral"
    litellm_params:
      model: "mistral/codestral-latest"
      tags: ["think", "review", "plan"]

  - model_name: "devstral"
    litellm_params:
      model: "mistral/devstral-latest"
      tags: ["think", "review", "plan"]

  - model_name: "devstral-medium"
    litellm_params:
      model: "mistral/devstral-medium-latest"
      tags: ["default", "background"]

  - model_name: "devstral-small"
    litellm_params:
      model: "mistral/devstral-small-latest"
      tags: ["default", "background"]

  - model_name: "ministral-14b"
    litellm_params:
      model: "mistral/ministral-14b-latest"
      tags: ["default", "background"]

  - model_name: "ministral-8b"
    litellm_params:
      model: "mistral/ministral-8b-latest"
      tags: ["default", "background"]

  - model_name: "ministral-3b"
    litellm_params:
      model: "mistral/ministral-3b-latest"
      tags: ["default", "background"]

  - model_name: "pixtral-large"
    litellm_params:
      model: "mistral/pixtral-large-latest"
      tags: ["think", "review", "plan"]

  - model_name: "open-mistral-nemo"
    litellm_params:
      model: "mistral/open-mistral-nemo"
      tags: ["default", "background"]

router_settings:
  enable_tag_filtering: true
  num_retries: 2

litellm_settings:
  drop_params: true
  set_verbose: false
  request_timeout: 120
  num_retries: 2

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"
